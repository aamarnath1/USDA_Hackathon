{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b73c77ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing our Packages\n",
    "#If anything doesn't work, use !pip install package_name\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import datasets\n",
    "import requests\n",
    "import re\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "from segments import SegmentsClient, SegmentsDataset\n",
    "from segments.utils import export_dataset, load_label_bitmap_from_url, get_semantic_bitmap\n",
    "from segments.huggingface import release2dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "66efa714",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data Cleaning?\n",
    "\n",
    "input_directory = '/Users/Arya/Documents/Data Science/Spring Hackathon/USDA_Hackathon/data/Path2/Path2-Model Training/Path2 Training Images'\n",
    "output_directory = '/Users/Arya/Documents/Data Science/Spring Hackathon/USDA_Hackathon/data/Path2/Path2-Model Training/Training-PNG'\n",
    "\n",
    "os.makedirs(output_directory, exist_ok=True)\n",
    "\n",
    "# List all files in the directory\n",
    "all_files = os.listdir(input_directory)\n",
    "all_files\n",
    "\n",
    "# Filter for .tif files\n",
    "tif_files = [file for file in all_files if file.endswith('.tif')]\n",
    "\n",
    "for tif_file in tif_files:\n",
    "    # Construct the full file path\n",
    "    full_path = os.path.join(input_directory, tif_file)\n",
    "    \n",
    "    with Image.open(full_path) as img:\n",
    "\n",
    "        base_name = os.path.splitext(tif_file)[0]\n",
    "        output_path_png = os.path.join(output_directory, f'{base_name}.png')\n",
    "        img.save(output_path_png, 'PNG')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3e234afb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing dataset...\n",
      "Preloading all samples. This may take a while...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|\u001b[38;2;255;153;0m██████████\u001b[0m| 40/40 [00:00<00:00, 218.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized dataset with 40 images.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#Loading Data from Segments.ai\n",
    "#!pip install --upgrade segments-ai\n",
    "\n",
    "\n",
    "api_key = \"7b8fe6e8c2ee6ebfc9a2df2a4397ab7d80375620\" #API KEY\n",
    "client = SegmentsClient(api_key)\n",
    "\n",
    "#datasets = client.get_datasets()\n",
    "#print(datasets)\n",
    "\n",
    "dataset_identifier = 'aamarnath1/USDA_Hackathon-clone'\n",
    "name = 'v1.0'\n",
    "release = client.get_release(dataset_identifier, name)\n",
    "dataset = SegmentsDataset(release, labelset='ground-truth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f6c5850a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "uuid='0f3f61c9-8bd0-4e1f-bea7-912070f7728e' name='v1.0' description='labelled' release_type='JSON' attributes=URL(url='https://segmentsai-prod.s3.amazonaws.com/releases/0f3f61c9-8bd0-4e1f-bea7-912070f7728e.json?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIA5RYRXRX2XBW5X4DZ%2F20240406%2Feu-west-2%2Fs3%2Faws4_request&X-Amz-Date=20240406T183243Z&X-Amz-Expires=86400&X-Amz-SignedHeaders=host&X-Amz-Signature=26e01f74d793452da2614fd52b49745a2e38f44cc53e5332fddf8cfd92a0202f') status='SUCCEEDED' created_at='2024-04-06T04:55:36.849670Z' samples_count=40\n"
     ]
    }
   ],
   "source": [
    "print(release)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b9deff48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<segments.dataset.SegmentsDataset object at 0x7fd453747550>\n"
     ]
    }
   ],
   "source": [
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "67c8a0b9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "32758e8e16a24742bf3dbcc0aa643a53",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/40 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca39d86a16224827a03a5aeae41ce6ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/40 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "hfdataset = release2dataset(release, download_images=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dbad8d94",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d89b150084d4d64bbe0aee5fe04b25d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/40 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Arya/opt/anaconda3/lib/python3.8/site-packages/datasets/features/image.py:341: UserWarning: Downcasting array dtype int64 to int32 to be compatible with 'Pillow'\n",
      "  warnings.warn(f\"Downcasting array dtype {dtype} to {dest_dtype} to be compatible with 'Pillow'\")\n"
     ]
    }
   ],
   "source": [
    "#Need to convert bitmaps into semantic bitmaps\n",
    "\n",
    "def convert_segmentation_bitmap(example):\n",
    "    return {\n",
    "        \"label.segmentation_bitmap\":\n",
    "            get_semantic_bitmap(\n",
    "                example[\"label.segmentation_bitmap\"],\n",
    "                example[\"label.annotations\"],\n",
    "            )\n",
    "    }\n",
    "\n",
    "\n",
    "semantic_dataset = hfdataset.map(\n",
    "    convert_segmentation_bitmap,\n",
    ")\n",
    "\n",
    "semantic_dataset = semantic_dataset.rename_column('image', 'pixel_values')\n",
    "semantic_dataset = semantic_dataset.rename_column('label.segmentation_bitmap', 'label.bitmap')\n",
    "semantic_dataset = semantic_dataset.remove_columns(['status'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "057fe601",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': Value(dtype='string', id=None),\n",
       " 'uuid': Value(dtype='string', id=None),\n",
       " 'status': Value(dtype='string', id=None),\n",
       " 'image': Image(decode=True, id=None),\n",
       " 'label.annotations': [{'id': Value(dtype='int32', id=None),\n",
       "   'category_id': Value(dtype='int32', id=None)}],\n",
       " 'label.segmentation_bitmap': Image(decode=True, id=None)}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hfdataset.features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "554e1e53",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': Value(dtype='string', id=None),\n",
       " 'uuid': Value(dtype='string', id=None),\n",
       " 'pixel_values': Image(decode=True, id=None),\n",
       " 'label.annotations': [{'id': Value(dtype='int32', id=None),\n",
       "   'category_id': Value(dtype='int32', id=None)}],\n",
       " 'label.bitmap': Image(decode=True, id=None)}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "semantic_dataset.features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "aa4ce308",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_process_bitmap(url):\n",
    "    response = requests.get(url)\n",
    "    bitmap_image = Image.open(BytesIO(response.content)).convert(\"L\")  # Convert to grayscale for simplicity\n",
    "    bitmap_array = np.array(bitmap_image)\n",
    "    return bitmap_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "9c126243",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<PIL.TiffImagePlugin.TiffImageFile image mode=I size=768x572 at 0x7FD3F0090C10>\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique pixel values in the bitmap: [0 1 2]\n",
      "I\n"
     ]
    }
   ],
   "source": [
    "print(bitmap_array)\n",
    "plt.imshow(bitmap_array)\n",
    "plt.show()\n",
    "\n",
    "# After obtaining the bitmap array\n",
    "unique_values = np.unique(bitmap_array)\n",
    "print(\"Unique pixel values in the bitmap:\", unique_values)\n",
    "print(bitmap_image.mode)  # Should typically be 'L' for 8-bit pixels, black and white"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f99aa4f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#OUTPUTTING PIXEL DIMNESIONS FROM BITMAP WITH DATA\n",
    "excel_data = pd.read_excel('/Users/Arya/Documents/Data Science/Spring Hackathon/USDA_Hackathon/data/Path2/Path2-Model Training/Path2 Data.xlsx')\n",
    "\n",
    "# Function to calculate ribeye area and fat thickness from bitmap\n",
    "def calculate_measurements_from_bitmap(bitmap):\n",
    "    ribeye_pixels = np.sum(bitmap == 1)\n",
    "    fat_thickness_pixels = np.sum(bitmap == 2)\n",
    "    \n",
    "    return ribeye_pixels, fat_thickness_pixels\n",
    "\n",
    "for sample in semantic_dataset:\n",
    "    \n",
    "    filename = sample['name']  # Assuming this is where the filename is stored\n",
    "    match = re.search(r'\\d+', filename)\n",
    "    if match:\n",
    "        carcass_id = int(match.group())\n",
    "    else:\n",
    "        # If no matching numeric part is found, skip this sample or handle it as needed\n",
    "        continue\n",
    "        \n",
    "    # Directly access the bitmap image\n",
    "    bitmap_image = sample['label.bitmap']  # Placeholder for direct access; adjust as needed\n",
    "    \n",
    "    # Assuming bitmap_image is already in a suitable format (numpy array) after your preprocessing steps\n",
    "    bitmap_array = bitmap_image\n",
    "    \n",
    "    # Calculate pixel counts for ribeye area and fat thickness\n",
    "    ribeye_pixels, fat_thickness_pixels = calculate_measurements_from_bitmap(bitmap_array)\n",
    "    \n",
    "    if carcass_id in excel_data['Carcass ID'].values:\n",
    "        excel_data.loc[excel_data['Carcass ID'] == carcass_id, 'Ribeye Area Pixels'] = ribeye_pixels\n",
    "        excel_data.loc[excel_data['Carcass ID'] == carcass_id, 'Fat Thickness Pixels'] = fat_thickness_pixels\n",
    "\n",
    "# After the loop, save the updated DataFrame to an Excel file\n",
    "excel_data.to_excel('/Users/Arya/Documents/Data Science/Spring Hackathon/USDA_Hackathon/data/Path2/Path2-Model Training/Data-Pixels.xlsx', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a1e0342d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load your Excel file into a DataFrame\n",
    "excel_data = pd.read_excel('/Users/Arya/Documents/Data Science/Spring Hackathon/USDA_Hackathon/data/Path2/Path2-Model Training/Path2 Data.xlsx')\n",
    "\n",
    "# Define your conversion factors (these need to be calibrated for your dataset)\n",
    "pixels_per_square_inch = 0.0001124  # Placeholder value\n",
    "pixels_per_inch = 0.0001124  # Placeholder value\n",
    "\n",
    "\n",
    "# Function to calculate ribeye area and fat thickness from bitmap\n",
    "def calculate_measurements_from_bitmap(bitmap):\n",
    "    ribeye_pixels = np.sum(bitmap == 1)\n",
    "    fat_thickness_pixels = np.sum(bitmap == 2)\n",
    "    \n",
    "    return ribeye_pixels, fat_thickness_pixels\n",
    "\n",
    "# Function to convert pixel measurements to real-world measurements\n",
    "def convert_pixels_to_measurements(ribeye_pixels, fat_thickness_pixels, pixels_per_square_inch, pixels_per_inch):\n",
    "    ribeye_area_sq_in = ribeye_pixels * pixels_per_square_inch\n",
    "    fat_thickness_in = fat_thickness_pixels * pixels_per_inch\n",
    "    \n",
    "    return ribeye_area_sq_in, fat_thickness_in\n",
    "\n",
    "\n",
    "for sample in semantic_dataset:\n",
    "    \n",
    "    filename = sample['name']  # Assuming this is where the filename is stored\n",
    "    match = re.search(r'\\d+', filename)\n",
    "    if match:\n",
    "        carcass_id = int(match.group())\n",
    "    else:\n",
    "        # If no matching numeric part is found, skip this sample or handle it as needed\n",
    "        continue\n",
    "        \n",
    "        \n",
    "    # Directly access the bitmap image; how to do this depends on the dataset object's methods\n",
    "    bitmap_image = sample['label.bitmap']\n",
    "    \n",
    "    # Example conversion from a PIL Image (if applicable)\n",
    "    if isinstance(bitmap_image, Image.Image):\n",
    "        bitmap_array = np.array(bitmap_image)\n",
    "    else:\n",
    "        # If 'bitmap_image' is already a numpy array or similar, you can directly use it\n",
    "        bitmap_array = bitmap_image  # Assuming it's already in the correct format\n",
    "    \n",
    "    # Calculate measurements\n",
    "    ribeye_pixels, fat_thickness_pixels = calculate_measurements_from_bitmap(bitmap_array)\n",
    "    area_sq_in, thickness_in = convert_pixels_to_measurements(ribeye_pixels, fat_thickness_pixels, pixels_per_square_inch, pixels_per_inch)\n",
    "    \n",
    "    # Assuming you can correlate the sample with an Excel row (e.g., via a unique identifier or filename)\n",
    "    # Update the Excel DataFrame (make sure to define and load excel_data DataFrame beforehand)\n",
    "    if carcass_id in excel_data['Carcass ID'].values:\n",
    "        excel_data.loc[excel_data['Carcass ID'] == carcass_id, 'Calculated Ribeye Area (sq in)'] = area_sq_in\n",
    "        excel_data.loc[excel_data['Carcass ID'] == carcass_id, 'Calculated Fat Thickness (in)'] = thickness_in\n",
    "\n",
    "# Save the updated DataFrame back to an Excel file\n",
    "excel_data.to_excel('/Users/Arya/Documents/Data Science/Spring Hackathon/USDA_Hackathon/data/Path2/Path2-Model Training/UpdatedData.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "710a888d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94d464a1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08352653",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "285cc4ad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98a0d0fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "for sample in dataset:\n",
    "    # Assuming you have a way to identify the URL or direct access to the bitmap\n",
    "    bitmap_url = sample['category_id']['label.bitmap']['url']\n",
    "    bitmap = load_label_bitmap_from_url(bitmap_url)\n",
    "    \n",
    "    # Calculate measurements from the bitmap\n",
    "    ribeye_pixels, fat_thickness_pixels = calculate_measurements_from_bitmap(bitmap)\n",
    "    area_sq_in, thickness_in = convert_pixels_to_measurements(ribeye_pixels, fat_thickness_pixels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9854c606",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load your Excel data\n",
    "excel_data = pd.read_excel('/Users/Arya/Documents/Data Science/Spring Hackathon/USDA_Hackathon/data/Path2/Path2-Model Training/Path2 Data.xlsx')  # Adjust the filename as necessary\n",
    "\n",
    "# Assuming 'segmentations' is a list of bitmap arrays corresponding to your Excel rows\n",
    "# This needs to be populated based on how you access your Segment AI dataset\n",
    "segmentations = [...]  # Populate this list with your bitmap data arrays\n",
    "\n",
    "for index, (row, bitmap_array) in enumerate(zip(excel_data.iterrows(), segmentations)):\n",
    "    _, row = row  # Extract the row data\n",
    "    \n",
    "    # Calculate pixel counts from the bitmap\n",
    "    ribeye_pixels, fat_thickness_pixels = calculate_pixel_counts(bitmap_array)\n",
    "    \n",
    "    # Convert pixel counts to real-world measurements\n",
    "    area_sq_in, thickness_in = convert_pixels_to_measurements(ribeye_pixels, fat_thickness_pixels)\n",
    "    \n",
    "    # Update the DataFrame with the calculated measurements\n",
    "    excel_data.at[index, 'Calculated Ribeye Area (sq in)'] = area_sq_in\n",
    "    excel_data.at[index, 'Calculated Fat Thickness (in)'] = thickness_in\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e796bf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data Pre-Processing - how can we change the image to seperate the fat + ribeye area?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b181d131",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2c13b4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80cf4f83",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Objective -> Classify the beef images "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
